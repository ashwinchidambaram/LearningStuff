======== Part 1: Data Preprocessing Notes [Python] ========

-- 2.15

We need to always remember the difference between the independent and dependent
variables. Always using the independent variables to predict the dependent
variables.

-- 2.16

Import the following libraries which are generally used:

  import numpy as np
      ## numpy is used to perform any mathematical function in python and and create alias 'np' to make it easy to refer to

  import matplotlib.pyplot as plt
      ## matplotlib.pyplot (matplotlib is main library, pyplot is sublibrary)

  import pandas as pd
      ## pandas library is best to import and manage data sets

-- 2.17
To store/read a dataset:

  dataset = pd.read_csv('DATASET_NAME')

To separate data into Dependent/Independent Variables:

  X = dataset.iloc[a, b].values
  Y = dataset.iloc[c, d].values

    ## Where a, c represent the lines/rows to be read; Where b, d represent the columns to be read
    ## By using ':' it signifies all, and we can restrict by doing ':-x' or 'x' where x is some number

-- 2.18

A class is the model of something we want to build. For example, if we make a
house construction plan that gathers the instructions on how to build a house,
then this construction plan is the class.

An object is an instance of the class. So if we take that same example of the
house construction plan, then an object is simply a house. A house (the object)
that was built by following the instructions of the construction plan (the class).
And therefore there can be many objects of the same class, because we can build
many houses from the construction plan.

A method is a tool we can use on the object to complete a specific action. So in
this same example, a tool can be to open the main door of the house if a guest
is coming. A method can also be seen as a function that is applied onto the object,
takes some inputs (that were defined in the class) and returns some output.

-- 2.19

One of the most common methods to dealing with missing data, is by replacing the
missing data point with the average of all other data values in the column.

To do so:

  from sklearn.impute import SimpleImputer
    ## sklearn AKA 'scikit learn' which has the ability to preprocess data
    ## Using SimpleImputer class which handles missing data

  imputer = SimpleImputer(missing_values = np.nan, strategy = 'mean')
    ## First we create the ruleset by which imputer will work
    ## Other than 'mean', we can also use 'median' or 'most_frequent'

  imputer = imputer.fit(X[a, b])
    ## Using imputer, we fit the X dataset in range [a, b] and set it to the imputer object
        ### The ':' represents all lines/rows as before
        ### When adding range for 'b' remember that we start with the lower bound, and end with one more than upper bound
        ### This is because the upper bound is always excluded

  X[a, b] = imputer.transform(X[a, b])
    ## We are applying the method of the imputer object 'transform' that replaces the missing data with the mean of the column

-- 2.20

Categorical Data: Any data that is represented as a category

We have to sort data into categorical because while doing ML, the computer will
not be able to interpret a proper meaning from arbitrary/categorical data.

If we were to simply sort categorical data, we could potentially run into issues
such as bias. In this section's example, we have categorical data {France, Spain
Germany} which are assigned values from 0-2 and as such have implicit bias from
differing values. This can be seen if we were to use the following code snippet:

  from sklearn.preprocessing import LabelEncoder
  labelencoder_X = LabelEncoder()
  X[:, 0] = labelencoder_X.fit_transform(X[:, 0])

To mitigate this, we would have to use **Dummy Encoding** which in essence assigns
values to each of the categorical data as 0 or 1 in a truth table like manner.
By doing this, we can reduce implicit bias that may be present. Visually what
happens is depicted below:

  Without Dummy Encoding:

    COUNTRY       ENCODED
    France          0
    Spain           2
    Germany         1
    Spain           2
    Germany         1
    France          0
    Spain           2
    France          0
    Germany         1
    France          0

  With Dummy Encoding:

    COUNTRY       FRANCE   GERMANY    SPAIN
    France          1         0         0
    Spain           0         0         1
    Germany         0         1         0
    Spain           0         0         1
    Germany         0         1         0
    France          1         0         0
    Spain           0         0         1
    France          1         0         0
    Germany         0         1         0
    France          1         0         0

-- 2.22

When working with a data set, we should always make sure to split it into a
Training set and a Test set.

The reason for this being that, by training a ML model too much using one data
set, it will begin to get good at rote memorization but worse with inference.
But by splitting into a Training and Test datasets, it will allow for better
accuracy from better inference algorithms. This is called "overfitting".

To apply this using code:

  from sklearn.model_selection import train_test_split
    ## Where sklearn represents scikit
    ## model_selection is the upgraded library from cross_validation
    ## and train_test_split is the library used to split a data set into train/test

  X_train, X_test, Y_train, Y_test = train_test_split(a, b, test_size = 0.2)
    ## Where X represents independent variables, Y represent dependent variables
    ## We specify the variables as either 'train' or 'test'
    ## Then we use 'train_test_split' to split dataset according to specification
      ### format: train_test_split(a, b, test_size = 0.2)
      ### test_size refers to the split created in decimal value (0.2 => 20%)

      NOTE: Generally using a test_size between 0.2, 0.25, or 0.3 is recommended

Doing so, in this example, splits our 10 data points into a test set containing
2 items and a training set containing 8.


####################################################################################################################################
NOTE: By highlighting a keyword and pressing 'Command I', we can inspect and understand syntax/usage
NOTE: Generally using a test_size between 0.2 and 0.3 is recommended
